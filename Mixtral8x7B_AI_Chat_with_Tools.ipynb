{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Chat with Tools using Mistral AI's Mixtral8x7B [Mixtral of Experts](https://mistral.ai/news/mixtral-of-experts/) Large Language Model\n",
        "\n",
        "## This notebook downloads and initializes Mistal AI's Mixtral8x7B [Mixtral of Experts](https://mistral.ai/news/mixtral-of-experts/) using the [Huggingface transformers](https://huggingface.co/docs/transformers/index) package along with facilitating Tool Usage and Chat Session Logic.\n",
        "\n",
        "The code is based on this [Pinecone Tutorial](https://www.pinecone.io/learn/mixtral-8x7b/) and extends it to include additional inference/memory improvements (4-bit/8-bit quantization and Flash Attention 2) along with full multi-step chat logic. Please check out their tutorial for a step-by-step walkthrough on the implementation.\n",
        "\n",
        "### Tools\n",
        "\n",
        "- \"Calculator\" - Enables Mixtral AI Assistant to perform math calculations by providing python code which is then run executed using exec() - **WARNING* Using exec() to execute arbitrary Python code can be dangerous, as it can execute any Python command. This might lead to security vulnerabilities, especially in a web-based environment like Google Colab. This notebook does not currently apply any sanitzation, filtering, or other safety measures to the code generated by the Mixtral model before code execution, so please be cautious and use at your own risk.**\n",
        "- \"Search\" - Enables Mixtral AI Assistant to Search the web for real-time information using DuckDuckGo\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFaOHFn4lQCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up\n",
        "\n",
        "## 1) Select GPU Runtime\n",
        "In the top left of Google Colab, select Runtime > Change runtime type > A100 GPU\n",
        "- If you are using the free version of Google Colab, A100 GPUs may not be an option, but you might be able to use a different GPU if you use the \"High-RAM\" Runtime Setting along with 4-bit quantization (set `load_in_4_bit=True`), but I've only tested this code using an A100 so far. I've been able to get reasonably fast inference speeds using 4-bit quantization and Flash Attention 2 (set `use_flash_attention_2=True`) with the single 40G A100 provided by Google Colab, but I have not been able to successfully load the model in higher precision yet. If you'd like to use a runtime with better hardware, click on the resources dropdown in the top right corner (where your GPU/Runtime Type is displayed) and select \"Connect to a custom GCE VM\"\n",
        "\n",
        "\n",
        "## 2) Install Dependencies\n",
        "Run the cell below to install all required package dependencies. After installations are complete, the runtime will automatically be killed to restart the kernel so that the changes take effect. Once the runtime has been killed, you can move onto step 3 below to download, setup, and inference the Mixtral model (Do not re-run the installation cell)\n",
        "\n"
      ],
      "metadata": {
        "id": "TPVUrcqKkvuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install Huggingface & dependencies\n",
        "\n",
        "from google.colab import output, files\n",
        "import os\n",
        "\n",
        "# Tooling Installaltions from the pinecone tutorial - https://www.pinecone.io/learn/mixtral-8x7b/\n",
        "!pip install -qU \\\n",
        "    transformers==4.36.1 \\\n",
        "    accelerate==0.25.0 \\\n",
        "    duckduckgo_search==4.1.0\n",
        "# Hugginface transformers module\n",
        "!pip install -U transformers\n",
        "# Enables 4-bit & 8-bit quantization\n",
        "!pip install -U pip install bitsandbytes\n",
        "# Enables Flash Attention 2\n",
        "!pip install flash-attn --no-build-isolation\n",
        "output.clear()\n",
        "\n",
        "print(\"Huggingface Transformers module downloaded successfully! Restarting Runtime...\")\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1y1cdhhiky5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Download and Initialize Mixtral Model\n",
        "\n",
        "Select your model settings, and then run the \"**Download & Initialize Model**\" cell to load the Mixtral model. You should only run this cell once to avoid reloading the model (it takes a while), and then you can run the Run AI Chat cell below it as many times as you want to initialize new chat sessions.\n",
        "\n",
        "### Settings\n",
        "1)  `use_instruct_model: bool = True` - If True, uses the instruct-finetuned Mixtral-8x7B-Instruct-v0.1 model. If False, uses the Mixtral-8x7B-v0.1 base model.\n",
        "- This notebook is designed for use with the Instruct model, so you will most likely get errors if you set `use_instruct_model: bool = False` as the base model is mainly just incorporated demonstration purposes. If you do want to use the base model, make sure to change the `sys_message: str = None` in the \"Run Mixtral8x7B AI Chat with Tools\" cell of Step 4 to your own text-completion prompt instead of None, which applies the default Instruct model with Tools system prompt (reference the default sys_message in the `first_prompt_instruction_format(query: str, sys_message: str = None)` function located in the \"Download & Initialize Mixtral8x7B Model\" cell in step 3 to ensure your new sys_message is compatible with this function)\n",
        "\n",
        "2) `load_in_4_bit: bool = True` - If True, loads the model in reduced precision with 4-bit quantization using bitsandbytes to drastically reduce memory requirements and inference time\n",
        "\n",
        "3) `load_in_8_bit: bool = False` - If True, loads the model in reduced precision with 8-bit quantization using bitsandbytes to significantly reduce memory requirements and inference time. If both load_in_4_bit and load_in_8_bit are True, it will default to loading in 4-bit precision\n",
        "\n",
        "4) `use_flash_attention_2: bool = True` - Use Flash Attention 2 to significantly reduce inference time\n",
        "\n",
        "---\n",
        "### Text-Generation Args\n",
        "The remaining settings are input arguments for Huggingface `transformers.pipeline()` text-generation pipeline. I recommend leaving them set to the default values unless you know what you're doing. For more details, please refer to the [Huggingface Pipelines Documentation](https://huggingface.co/docs/transformers/main_classes/pipelines)\n",
        "- `temperature: float = 0.1` - 'Randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "- `top_p: float = 0.15` - Select from top tokens whose probability add up to top_p\n",
        "- `top_k: int = 0` - Select from top top_k tokens (because zero, relies on top_p)\n",
        "- `do_sample: bool = True` - I'm not really sure what this is tbh, but Transformers was giving a warning saying that this needs to be set to True since top_k is set\n",
        "- `max_new_tokens: int = 512` - Maximum number of tokens to generate per response\n",
        "- `repetition_penalty: float = 1.1` - Penalizes the model for repitive text generation. If the output begins repeating, increase this parameter\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AQ5nkCe8h35L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download & Initialize Mixtral8x7B Model\n",
        "\n",
        "# Model Settings\n",
        "use_instruct_model = True # @param {type:\"boolean\"}\n",
        "load_in_4_bit = True # @param {type:\"boolean\"}\n",
        "load_in_8_bit = False # @param {type:\"boolean\"}\n",
        "use_flash_attention_2 = True # @param {type:\"boolean\"}\n",
        "# Huggingface transformers.pipeline() Args\n",
        "temperature = 0.1 # @param {type:\"number\"}\n",
        "top_p = 0.15 # @param {type:\"number\"}\n",
        "top_k = 0 # @param {type:\"integer\"}\n",
        "do_sample = True # @param {type:\"boolean\"}\n",
        "max_new_tokens = 512 # @param {type:\"integer\"}\n",
        "repetition_penalty = 1.1  # @param {type:\"number\"}\n",
        "\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import json\n",
        "import datetime\n",
        "from duckduckgo_search import DDGS\n",
        "import os\n",
        "from google.colab import output\n",
        "import logging\n",
        "import io\n",
        "\n",
        "# Set the logging level to suppress warnings\n",
        "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
        "logging.getLogger('bitsandbytes').setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "# Select Model Version\n",
        "if use_instruct_model:\n",
        "  model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "else:\n",
        "  model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if load_in_4_bit:\n",
        "  if use_flash_attention_2:\n",
        "    # Load model in 4-bit precision with Flash Attention 2\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                                  pretrained_model_name_or_path = model_id,\n",
        "                                  load_in_4bit=True,\n",
        "                                  attn_implementation=\"flash_attention_2\"\n",
        "                                )\n",
        "  else:\n",
        "    # Load model in 8-bit precision\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                                  pretrained_model_name_or_path = model_id,\n",
        "                                  load_in_4bit=True\n",
        "                                )\n",
        "\n",
        "elif load_in_8_bit:\n",
        "  if use_flash_attention_2:\n",
        "    # Load model in 8-bit precision with Flash Attention 2\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                                  pretrained_model_name_or_path = model_id,\n",
        "                                  load_in_8bit=True,\n",
        "                                  attn_implementation=\"flash_attention_2\"\n",
        "                                )\n",
        "  else:\n",
        "    # Load model in 8-bit precision\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                                  pretrained_model_name_or_path = model_id,\n",
        "                                  load_in_8bit=True\n",
        "                                )\n",
        "\n",
        "else:\n",
        "  if use_flash_attention_2:\n",
        "    # Load model in full precision with Flash Attention 2\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                                  pretrained_model_name_or_path = model_id,\n",
        "                                  attn_implementation=\"flash_attention_2\"\n",
        "                                )\n",
        "  else:\n",
        "    # Load model in full precision\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                                  pretrained_model_name_or_path = model_id\n",
        "                                )\n",
        "\n",
        "\n",
        "\n",
        "# Create Huggingface Text Generation Pipeline\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,  # if using langchain set True\n",
        "    task=\"text-generation\",\n",
        "    # we pass model parameters here too\n",
        "    temperature=temperature,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    top_p=top_p,  # select from top tokens whose probability add up to 15%\n",
        "    top_k=top_k,  # select from top 0 tokens (because zero, relies on top_p)\n",
        "    do_sample=do_sample, # Transformers warning says I need to set this to True since top_k is set\n",
        "    max_new_tokens=max_new_tokens,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=repetition_penalty  # if output begins repeating increase\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Prints debug_message only if env variable \"DEBUG\".upper() == True\n",
        "def print_debug_message(debug_message: str):\n",
        "  if os.environ.get(\"DEBUG\") and os.environ.get(\"DEBUG\").upper() == \"TRUE\":\n",
        "    print(f\"\\n\\n\\n##### Debug Message ######\\n {debug_message}\\n##### End Debug Message ######\\n\\n\\n\")\n",
        "\n",
        "# Sanitize generated_text string for when the model directly\n",
        "# returned its response message rather than a valid action dict json\n",
        "# string for its response as intended so we have to format it manually\n",
        "# the action dict json string manually and must remove any potential newlines\n",
        "# and stuff from its message to ensure json parsability\n",
        "def sanitize_text_for_json(text: str):\n",
        "    # Strip whitespace and control characters from both ends of the text\n",
        "    sanitized_text = text.strip()\n",
        "\n",
        "    # Replace internal control characters like newlines with spaces\n",
        "    sanitized_text = sanitized_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
        "\n",
        "    return sanitized_text\n",
        "\n",
        "# Set up first user query formatted with initial system prompt (tool\n",
        "# use instructions included in default system prompt)\n",
        "def first_prompt_instruction_format(query: str, sys_message: str = None):\n",
        "\n",
        "    if sys_message is None:\n",
        "      sys_message = \"\"\"You are a helpful AI assistant, you are an agent capable of using a variety of tools to answer a question. Here are a few of the tools available to you:\n",
        "\n",
        "      - Calculator: the calculator should be used whenever you need to perform a calculation, no matter how simple. It uses Python so make sure to write complete Python code required to perform the calculation required and make sure the Python returns your answer to the `output` variable.\n",
        "      - Search: the search tool should be used whenever you need to find information. It can be used to find information about everything\n",
        "      - Final Answer: the final answer tool must be used to respond to the user. You must use this when you have decided on an answer.\n",
        "\n",
        "      To use these tools you must always respond in JSON format containing `\"tool_name\"` and `\"input\"` key-value pairs. For example, to answer the question, \"what is the square root of 51?\" you must use the calculator tool like so:\n",
        "\n",
        "      ```json\n",
        "      {\n",
        "          \"tool_name\": \"Calculator\",\n",
        "          \"input\": \"from math import sqrt; output = sqrt(51)\"\n",
        "      }\n",
        "      ```\n",
        "\n",
        "      Or to answer the question \"who is the current president of the USA?\" you must respond:\n",
        "\n",
        "      ```json\n",
        "      {\n",
        "          \"tool_name\": \"Search\",\n",
        "          \"input\": \"current president of USA\"\n",
        "      }\n",
        "      ```\n",
        "\n",
        "      Remember, even when answering to the user, you must still use this JSON format! If you'd like to ask how the user is doing you must write:\n",
        "\n",
        "      ```json\n",
        "      {\n",
        "          \"tool_name\": \"Final Answer\",\n",
        "          \"input\": \"How are you today?\"\n",
        "      }\n",
        "      ```\n",
        "\n",
        "      Let's get started. The users query is as follows.\n",
        "      \"\"\"\n",
        "      # note, don't \"</s>\" to the end\n",
        "\n",
        "    return f'<s> [INST] {sys_message} [/INST]\\nUser: {query}\\nAssistant: '#```json\\n{{\\n\"tool_name\": '\n",
        "\n",
        "\n",
        "def extract_first_json(text: str, debug: bool = False):\n",
        "    # Find the first and last braces to extract the JSON object\n",
        "    # This will be more robust against irregular formatting\n",
        "    start_index = text.find('{')\n",
        "    end_index = text.rfind('}')  # Get the last closing brace\n",
        "\n",
        "    if start_index == -1 or end_index == -1 or end_index < start_index:\n",
        "        return None  # Return None if valid JSON braces are not found\n",
        "\n",
        "    # Extract the substring that forms the JSON object\n",
        "    json_str = text[start_index:end_index + 1]\n",
        "\n",
        "    # Replace newline characters and other potential issues\n",
        "    #json_str = json_str.replace('\\n', '\\\\n').replace('\\r', '\\\\r').replace('\\t', '\\\\t')\n",
        "\n",
        "    return json_str\n",
        "\n",
        "def format_output(text: str):\n",
        "    print_debug_message(f\"format_output(): initial text: {text}\\n\")\n",
        "    full_json_str = extract_first_json(text)\n",
        "\n",
        "    if full_json_str is None:\n",
        "        print_debug_message(f\"format_output(): No valid JSON found calling extract_first_json() with text - {text}\")\n",
        "        return None\n",
        "\n",
        "    print_debug_message(f\"format_output(): full_json_str after extract_first_json()call - {full_json_str}\\n\")\n",
        "\n",
        "    try:\n",
        "        return json.loads(full_json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print_debug_message(f\"format_output(): Error decoding JSON from json.loads(full_json_str) with text - {text}\\nand full_json_str - {full_json_str}\\nError Message - {e}\")\n",
        "        return None  # Handle the error as needed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Processes the action dict created by the format_output() function to execute\n",
        "# the selected tool or provide the AI's final response based on the value of\n",
        "# action[\"tool_name\"] (if tool name isn't recognized, it will assume it's a\n",
        "# \"Final Answer\" action)\n",
        "def use_tool(action: dict):\n",
        "    tool_name = action[\"tool_name\"]\n",
        "    if tool_name == \"Final Answer\":\n",
        "        is_tool_response = False\n",
        "        return f\"\\n\\nAI Assistant: {action['input']}\", is_tool_response\n",
        "    elif tool_name == \"Calculator\":\n",
        "        print(\"\\nUsing Calculator...\\n\")\n",
        "        # Create a dictionary to serve as a local namespace for exec\n",
        "        local_namespace = {}\n",
        "\n",
        "        # Execute the code within the local namespace\n",
        "        exec(action[\"input\"], {}, local_namespace)\n",
        "\n",
        "        # Access the value of 'result' from the local namespace\n",
        "        exec_result = local_namespace.get('output', None)\n",
        "        is_tool_response = True\n",
        "        return f\"\\n\\nTool Output: {exec_result}\", is_tool_response\n",
        "    elif tool_name == \"Search\":\n",
        "        print(f\"\\nSearching the Web for {action['input']}...\\n\")\n",
        "        contexts = []\n",
        "        with DDGS() as ddgs:\n",
        "            results = ddgs.text(\n",
        "                action[\"input\"],\n",
        "                region=\"wt-wt\", safesearch=\"on\",\n",
        "                max_results=3\n",
        "            )\n",
        "            for r in results:\n",
        "                contexts.append(r['body'])\n",
        "        info = \"\\n---\\n\".join(contexts)\n",
        "        is_tool_response = True\n",
        "        return f\"\\n\\nTool Output: {info}\", is_tool_response\n",
        "    else:\n",
        "        # otherwise just assume final answer\n",
        "        is_tool_response = False\n",
        "        return f\"\\n\\nAI Assistant: {action['input']}\", is_tool_response\n",
        "\n",
        "# Takes a full input_prompt (with prior conversation history included),\n",
        "# queries the Mixtral model, processes the text-generation response, and\n",
        "# itteratively executes Tool Usage until a \"Final Answer\" answer is recieved.\n",
        "# Then, returns both the response_message to be displayed and the\n",
        "# new input_prompt with all new messages appended\n",
        "def handle_message(input_prompt):\n",
        "\n",
        "  response = generate_text(input_prompt)\n",
        "  generated_text = response[0]['generated_text']\n",
        "  print_debug_message(f\"handle_message(): initial generated_text: {generated_text}\\n\")\n",
        "\n",
        "  # If it fails to load json, it probably just responded\n",
        "  # directly without the dict, so build a dict out of it\n",
        "  # in a try/except clause\n",
        "  try:\n",
        "      action = format_output(generated_text)\n",
        "      if action is None:\n",
        "        # if action is None, the formatting was wrong, meaning the model probably didn't\n",
        "        # wrap its response message in an action dict, so raise an error to let except\n",
        "        # block reformat it into a \"Final Answer\" action dict json string\n",
        "        raise Exception(\"Failed to parse json from generated_text to creare action dict.\")\n",
        "      response_message, is_tool_response = use_tool(action)\n",
        "  except Exception as e:\n",
        "\n",
        "      # If formatting the output and using the tool fails, then the model\n",
        "      # probably didn't use action dict json string formatting, so just\n",
        "      # assume its message was intended to be a direct message to the user\n",
        "      # and reformat it as a \"Final Answer\" action\n",
        "      print_debug_message(f\"handle_message(): Exception Block triggered for first text-generation's format_output()/use_tool() call! Exception - {e}\")\n",
        "\n",
        "      # Since the model probably sent direct response message rather than formatting it as\n",
        "      # an action dict json string, we need to remove any newlines, whitespace, etc\n",
        "      # to ensure the new action dict input value is json parsable in the new_generated_text\n",
        "      # json string\n",
        "      generated_text = sanitize_text_for_json(text = generated_text)\n",
        "      # Now format the response into a valid action dict json string\n",
        "      new_generated_text = \"\"\"\n",
        "      ```json\n",
        "      {\n",
        "          \"tool_name\": \"Final Answer\",\n",
        "          \"input\": \"\"\" + f\"\\\"{generated_text}\\\"\"+ \"\"\"\n",
        "      }\n",
        "      ```\n",
        "      \"\"\"\n",
        "      generated_text = new_generated_text\n",
        "      action = format_output(generated_text)\n",
        "      response_message, is_tool_response = use_tool(action)\n",
        "  # Add Initial Assistant Response to the prompt\n",
        "  input_prompt += generated_text\n",
        "\n",
        "  # If response is tool response, add the response message\n",
        "  # to the prompt and query again, return the full new\n",
        "  # input_prompt along with the response_message to be displayed\n",
        "  # to the user\n",
        "  while is_tool_response:\n",
        "    input_prompt += response_message + \"\\n\\nAI Assistant: \"\n",
        "    response = generate_text(input_prompt)\n",
        "    generated_text = response[0]['generated_text']\n",
        "    print_debug_message(f\"handle_message(): while is_tool_response generated_text : {generated_text}\")\n",
        "\n",
        "    # If it fails to load json, it probably just responded\n",
        "    # directly without the dict, so build a dict out of it\n",
        "    # in a try/except clause\n",
        "    try:\n",
        "        action = format_output(generated_text)\n",
        "        if action is None:\n",
        "          # if action is None, the formatting was wrong, meaning the model probably didn't\n",
        "          # wrap its response message in an action dict, so raise an error to let except\n",
        "          # block reformat it into a \"Final Answer\" action dict json string\n",
        "          raise Exception(\"Failed to parse json from generated_text to creare action dict.\")\n",
        "        response_message, is_tool_response = use_tool(action)\n",
        "    except Exception as e:\n",
        "\n",
        "        # If formatting the output and using the tool fails, then the model\n",
        "        # probably didn't use action dict json string formatting, so just\n",
        "        # assume its message was intended to be a direct message to the user\n",
        "        # and reformat it as a \"Final Answer\" action\n",
        "        print_debug_message(f\"handle_message(): Exception Block triggered for 'while is_tool_response' loop format_output()/use_tool() call! Exception - {e}\")\n",
        "\n",
        "        # Since the model probably sent direct response message rather than formatting it as\n",
        "        # an action dict json string, we need to remove any newlines, whitespace, etc\n",
        "        # to ensure the new action dict input value is json parsable in the new_generated_text\n",
        "        # json string\n",
        "        generated_text = sanitize_text_for_json(text = generated_text)\n",
        "        # Now format the response into a valid action dict json string\n",
        "        new_generated_text = \"\"\"\n",
        "        ```json\n",
        "        {\n",
        "            \"tool_name\": \"Final Answer\",\n",
        "            \"input\": \"\"\" + f\"\\\"{generated_text}\\\"\"+ \"\"\"\n",
        "        }\n",
        "        ```\n",
        "        \"\"\"\n",
        "        generated_text = new_generated_text\n",
        "        action = format_output(generated_text)\n",
        "        response_message, is_tool_response = use_tool(action)\n",
        "    # Add the new is_tool_response_loop generated_text to the prompt\n",
        "    input_prompt += generated_text\n",
        "\n",
        "\n",
        "\n",
        "  return response_message, input_prompt\n",
        "\n",
        "# Initializes AI Chat orchestrating user inputs and multi-step chat logic\n",
        "def run_ai_chat(sys_message: str = None, max_user_messages: int = 10, chat_log_dir: str = \"logs/\"):\n",
        "\n",
        "    # Create Chat Log Filepath to save convo history\n",
        "    now = datetime.datetime.now()\n",
        "    formatted_datetime = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "    if not os.path.exists(chat_log_dir):\n",
        "      os.makedirs(chat_log_dir)\n",
        "    chat_log_file = f\"{chat_log_dir}User_Conversation_{formatted_datetime}.txt\"\n",
        "\n",
        "    # Begin AI Assistant Chat\n",
        "    output.clear()\n",
        "    print(\"\\n\\nBeginning AI Assistant Chat. Type 'exit' at any time to end the chat\\n\\n\\n\")\n",
        "    print(\"\\n\\n\")\n",
        "    user_message = input(\"User: \")\n",
        "    print(\"\\n\\n\")\n",
        "    input_prompt = first_prompt_instruction_format(user_message, sys_message = sys_message)\n",
        "    response_message, input_prompt = handle_message(\n",
        "                                          input_prompt\n",
        "                                        )\n",
        "\n",
        "    print(response_message)\n",
        "    print(\"\\n\\n\")\n",
        "    # Save Chat Log\n",
        "    with open(chat_log_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(input_prompt)\n",
        "    # Start at 1, not 0, cuz they already sent the first message\n",
        "    for i in range(1, max_user_messages):\n",
        "      print(\"\\n\\n\")\n",
        "      user_message = input(\"User: \")\n",
        "      print(\"\\n\\n\")\n",
        "      input_prompt += \"\\nUser: \" + user_message\n",
        "\n",
        "      if user_message.upper() == 'EXIT':\n",
        "        input_prompt += \"\\n\\n\\nChat Ended. Have a great day! (:\"\n",
        "        print(\"\\n\\n\\nChat Ended. Have a great day! (:\\n\\n\")\n",
        "        # Save Chat Log\n",
        "        with open(chat_log_file, 'w', encoding='utf-8') as file:\n",
        "            file.write(input_prompt)\n",
        "        return input_prompt\n",
        "      input_prompt += f\"\\nUser: {user_message}\"\n",
        "      response_message, input_prompt = handle_message(\n",
        "                                          input_prompt\n",
        "                                        )\n",
        "\n",
        "      print(response_message)\n",
        "      print(\"\\n\\n\")\n",
        "      # Save Chat Log\n",
        "      with open(chat_log_file, 'w', encoding='utf-8') as file:\n",
        "            file.write(input_prompt)\n",
        "\n",
        "    print(\"\\n\\n\\nChat Ended. Have a great day! (:\")\n",
        "    # Save Chat Log\n",
        "    with open(chat_log_file, 'w', encoding='utf-8') as file:\n",
        "            file.write(input_prompt)\n",
        "    return input_prompt\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\nMixtral-8x7B-Instruct Model Loaded Successfully!\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tcFSb5rejKzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Run Mixtral AI Chat\n",
        "\n",
        "After the model has been downloaded & initialized, run the cell below to start your chat session\n",
        "### Settings\n",
        "- `max_user_messages: int = 20` - Maximum number of messages the user can send in a given AI Chat session. This is used to prevent conversations from becoming too long to fit within the model's maximum sequence length\n",
        "- `debug_mode: bool = False` - Print debug messages\n"
      ],
      "metadata": {
        "id": "rfVLoMBRxlpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run Mixtral8x7B AI Chat with Tools\n",
        "\n",
        "# How many total messages the user can send in a given AI Chat session\n",
        "# before it automatically ends the conversation. This prevents\n",
        "# conversations from becoming too long to fit within the model's maximum\n",
        "# sequence length\n",
        "max_user_messages = 20 # @param {type:\"integer\"}\n",
        "debug_mode = False # @param {type:\"boolean\"}\n",
        "\n",
        "# Set sys_message to None to use default tool-enabled system prompt (default\n",
        "# system prompt is located in the first_prompt_instruction_format() function\n",
        "# of the \"Download & Initialize Mixtral8x7B Model\" cell above)\n",
        "sys_message = None, # Setting \"None\"\n",
        "\n",
        "# Set \"DEBUG\" env variable to toggle print_debug_message() function printing\n",
        "if debug_mode:\n",
        "  os.environ['DEBUG'] = 'TRUE'\n",
        "else:\n",
        "  os.environ['DEBUG'] = 'FALSE'\n",
        "\n",
        "# Creates a chat_log_dir directory to save a .txt file of the full\n",
        "# chat conversation for each AI Chat session (including system prompt,\n",
        "# Tool Calls, Tool Outputs, etc) to help with debugging\n",
        "chat_log_dir = \"logs/\"\n",
        "\n",
        "# Set the logging level to suppress warnings\n",
        "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
        "logging.getLogger('bitsandbytes').setLevel(logging.ERROR)\n",
        "\n",
        "input_prompt = run_ai_chat(\n",
        "                  sys_message = sys_message,\n",
        "                  max_user_messages = max_user_messages,\n",
        "                  chat_log_dir = chat_log_dir\n",
        "                )\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SBUFYBDuxcgw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "A100",
      "machine_shape": "hm",
      "collapsed_sections": [
        "TPVUrcqKkvuG",
        "AQ5nkCe8h35L",
        "rfVLoMBRxlpp"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}