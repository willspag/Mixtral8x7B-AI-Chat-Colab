{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Enhancing AI Chat with Mistral AI's Mixtral8x7B Large Language Model\n",
        "\n",
        "## Overview\n",
        "This notebook is your guide to leveraging Mistral AI's advanced Mixtral8x7B [Mixtral of Experts](https://mistral.ai/news/mixtral-of-experts/) Large Language Model. It utilizes the [Huggingface transformers](https://huggingface.co/docs/transformers/index) package to provide an interactive and powerful AI assistant.\n",
        "\n",
        "Inspired by the comprehensive [Pinecone Tutorial](https://www.pinecone.io/learn/mixtral-8x7b/), this setup includes advanced features like 4-bit/8-bit quantization and Flash Attention 2 for improved efficiency and multi-step chat logic for a seamless interaction experience.\n",
        "\n",
        "### Tools\n",
        "\n",
        "- **Calculator**: A feature for the Mixtral AI Assistant to execute mathematical calculations. **Note**: This uses the `exec()` function, allowing execution of arbitrary Python code, which could pose security risks in environments like Google Colab.\n",
        "- **Search**: This enables the AI Assistant to perform real-time web searches using DuckDuckGo.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFaOHFn4lQCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Instructions\n",
        "\n",
        "## 1) GPU Runtime Selection\n",
        "In Google Colab, go to Runtime > Change runtime type > Select A100 GPU (if available). For the free version of Google Colab, where A100 might not be available, opt for a 'High-RAM' Runtime with 4-bit quantization (`load_in_4_bit=True`). Note: This has been tested on A100 GPU using 4-bit quantization and Flash Attention 2 (`use_flash_attention_2=True`). For more powerful hardware, use the 'Connect to a custom GCE VM' option.\n",
        "\n",
        "## 2) Installing Dependencies\n",
        "Execute the following cell to install necessary packages. After installation, the runtime will restart automatically. Do not rerun the installation cell; proceed to step 3 for downloading and setting up the Mixtral model.\n",
        "\n"
      ],
      "metadata": {
        "id": "TPVUrcqKkvuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install Huggingface & dependencies\n",
        "\n",
        "from google.colab import output, files\n",
        "import os\n",
        "\n",
        "# Installing tooling from Pinecone tutorial and additional dependencies\n",
        "!pip install -qU transformers==4.36.1 accelerate==0.25.0 duckduckgo_search==4.1.0\n",
        "# For 4-bit & 8-bit quantization\n",
        "!pip install -U bitsandbytes\n",
        "# For Flash Attention 2\n",
        "!pip install flash-attn --no-build-isolation\n",
        "output.clear()\n",
        "\n",
        "print(\"Dependencies installed successfully. Restarting Runtime...\")\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1y1cdhhiky5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Download and Initialize the Mixtral Model\n",
        "\n",
        "Set up the Mixtral model by selecting your preferences below, then run the '**Download & Initialize Model**' cell. Remember to only run this cell once to avoid time-consuming reloads. For initiating chat sessions, use the 'Run AI Chat' cell.\n",
        "\n",
        "### Model Settings\n",
        "- `use_instruct_model: bool = True`: Enables the instruct-finetuned Mixtral-8x7B-Instruct-v0.1 model. Set to `False` for the base model, but note this notebook is optimized for the Instruct model. For base model usage, modify `sys_message: str` in the 'Run Mixtral8x7B AI Chat with Tools' cell of Step 4 with your text-completion prompt.\n",
        "\n",
        "- `load_in_4_bit: bool = True`: Activates 4-bit quantization for reduced memory and faster inference.\n",
        "\n",
        "- `load_in_8_bit: bool = False`: Enables 8-bit quantization. If both 4-bit and 8-bit are true, 4-bit takes precedence.\n",
        "\n",
        "- `use_flash_attention_2: bool = True`: Utilizes Flash Attention 2 for faster inference.\n",
        "\n",
        "---\n",
        "### Text-Generation Arguments\n",
        "Default settings for Huggingface `transformers.pipeline()` are recommended unless you have specific requirements. For detailed information, refer to the [Huggingface Pipelines Documentation](https://huggingface.co/docs/transformers/main_classes/pipelines).\n",
        "- `temperature: float = 0.1`: Controls output randomness. Range: 0.0 (min) to 1.0 (max).\n",
        "- `top_p: float = 0.15`: Chooses from top tokens cumulatively adding up to `top_p`.\n",
        "- `top_k: int = 0`: Selects from top `top_k` tokens. Zero value means reliance on `top_p`.\n",
        "- `do_sample: bool = True`: Necessary for `top_k` usage, although its exact function is unclear.\n",
        "- `max_new_tokens: int = 512`: Limits the number of generated tokens per response.\n",
        "- `repetition_penalty: float = 1.1`: Discourages repetitive text. Increase if repetition occurs.\n",
        "\n"
      ],
      "metadata": {
        "id": "AQ5nkCe8h35L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download & Initialize Mixtral8x7B Model\n",
        "\n",
        "# Model Settings\n",
        "use_instruct_model = True # @param {type:\"boolean\"}\n",
        "load_in_4_bit = True # @param {type:\"boolean\"}\n",
        "load_in_8_bit = False # @param {type:\"boolean\"}\n",
        "use_flash_attention_2 = True # @param {type:\"boolean\"}\n",
        "# Huggingface transformers.pipeline() Args\n",
        "temperature = 0.1 # @param {type:\"number\"}\n",
        "top_p = 0.15 # @param {type:\"number\"}\n",
        "top_k = 0 # @param {type:\"integer\"}\n",
        "do_sample = True # @param {type:\"boolean\"}\n",
        "max_new_tokens = 512 # @param {type:\"integer\"}\n",
        "repetition_penalty = 1.1  # @param {type:\"number\"}\n",
        "\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import json\n",
        "import datetime\n",
        "from duckduckgo_search import DDGS\n",
        "import os\n",
        "from google.colab import output\n",
        "import logging\n",
        "import io\n",
        "\n",
        "# Set the logging level to suppress warnings\n",
        "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
        "logging.getLogger('bitsandbytes').setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "# Select Model Version\n",
        "if use_instruct_model:\n",
        "  model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "else:\n",
        "  model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if load_in_4_bit:\n",
        "  if use_flash_attention_2:\n",
        "    # Load model in 4-bit precision with Flash Attention 2\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                                  pretrained_model_name_or_path = model_id,\n",
        "                                  load_in_4bit=True,\n",
        "                                  attn_implementation=\"flash_attention_2\"\n",
        "                                )\n",
        "  else:\n",
        "    # Load model in 8-bit precision\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                                  pretrained_model_name_or_path = model_id,\n",
        "                                  load_in_4bit=True\n",
        "                                )\n",
        "\n",
        "elif load_in_8_bit:\n",
        "  if use_flash_attention_2:\n",
        "    # Load model in 8-bit precision with Flash Attention 2\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                                  pretrained_model_name_or_path = model_id,\n",
        "                                  load_in_8bit=True,\n",
        "                                  attn_implementation=\"flash_attention_2\"\n",
        "                                )\n",
        "  else:\n",
        "    # Load model in 8-bit precision\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                                  pretrained_model_name_or_path = model_id,\n",
        "                                  load_in_8bit=True\n",
        "                                )\n",
        "\n",
        "else:\n",
        "  if use_flash_attention_2:\n",
        "    # Load model in full precision with Flash Attention 2\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                                  pretrained_model_name_or_path = model_id,\n",
        "                                  attn_implementation=\"flash_attention_2\"\n",
        "                                )\n",
        "  else:\n",
        "    # Load model in full precision\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                                  pretrained_model_name_or_path = model_id\n",
        "                                )\n",
        "\n",
        "\n",
        "\n",
        "# Create Huggingface Text Generation Pipeline\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,  # if using langchain set True\n",
        "    task=\"text-generation\",\n",
        "    # we pass model parameters here too\n",
        "    temperature=temperature,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    top_p=top_p,  # select from top tokens whose probability add up to 15%\n",
        "    top_k=top_k,  # select from top 0 tokens (because zero, relies on top_p)\n",
        "    do_sample=do_sample, # Transformers warning says I need to set this to True since top_k is set\n",
        "    max_new_tokens=max_new_tokens,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=repetition_penalty  # if output begins repeating increase\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Prints debug_message only if env variable \"DEBUG\".upper() == True\n",
        "def print_debug_message(debug_message: str):\n",
        "  if os.environ.get(\"DEBUG\") and os.environ.get(\"DEBUG\").upper() == \"TRUE\":\n",
        "    print(f\"\\n\\n\\n##### Debug Message ######\\n {debug_message}\\n##### End Debug Message ######\\n\\n\\n\")\n",
        "\n",
        "# Sanitize generated_text string for when the model directly\n",
        "# returned its response message rather than a valid action dict json\n",
        "# string for its response as intended so we have to format it manually\n",
        "# the action dict json string manually and must remove any potential newlines\n",
        "# and stuff from its message to ensure json parsability\n",
        "def sanitize_text_for_json(text: str):\n",
        "    # Strip whitespace and control characters from both ends of the text\n",
        "    sanitized_text = text.strip()\n",
        "\n",
        "    # Replace internal control characters like newlines with spaces\n",
        "    sanitized_text = sanitized_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
        "\n",
        "    return sanitized_text\n",
        "\n",
        "# Set up first user query formatted with initial system prompt (tool\n",
        "# use instructions included in default system prompt)\n",
        "def first_prompt_instruction_format(query: str, sys_message: str = None):\n",
        "\n",
        "    if sys_message is None:\n",
        "      sys_message = \"\"\"You are a helpful AI assistant, you are an agent capable of using a variety of tools to answer a question. Here are a few of the tools available to you:\n",
        "\n",
        "      - Calculator: the calculator should be used whenever you need to perform a calculation, no matter how simple. It uses Python so make sure to write complete Python code required to perform the calculation required and make sure the Python returns your answer to the `output` variable.\n",
        "      - Search: the search tool should be used whenever you need to find information. It can be used to find information about everything\n",
        "      - Final Answer: the final answer tool must be used to respond to the user. You must use this when you have decided on an answer.\n",
        "\n",
        "      To use these tools you must always respond in JSON format containing `\"tool_name\"` and `\"input\"` key-value pairs. For example, to answer the question, \"what is the square root of 51?\" you must use the calculator tool like so:\n",
        "\n",
        "      ```json\n",
        "      {\n",
        "          \"tool_name\": \"Calculator\",\n",
        "          \"input\": \"from math import sqrt; output = sqrt(51)\"\n",
        "      }\n",
        "      ```\n",
        "\n",
        "      Or to answer the question \"who is the current president of the USA?\" you must respond:\n",
        "\n",
        "      ```json\n",
        "      {\n",
        "          \"tool_name\": \"Search\",\n",
        "          \"input\": \"current president of USA\"\n",
        "      }\n",
        "      ```\n",
        "\n",
        "      Remember, even when answering to the user, you must still use this JSON format! If you'd like to ask how the user is doing you must write:\n",
        "\n",
        "      ```json\n",
        "      {\n",
        "          \"tool_name\": \"Final Answer\",\n",
        "          \"input\": \"How are you today?\"\n",
        "      }\n",
        "      ```\n",
        "\n",
        "      Let's get started. The users query is as follows.\n",
        "      \"\"\"\n",
        "      # note, don't \"</s>\" to the end\n",
        "\n",
        "    return f'<s> [INST] {sys_message} [/INST]\\nUser: {query}\\nAssistant: '#```json\\n{{\\n\"tool_name\": '\n",
        "\n",
        "\n",
        "def extract_first_json(text: str, debug: bool = False):\n",
        "    # Find the first and last braces to extract the JSON object\n",
        "    # This will be more robust against irregular formatting\n",
        "    start_index = text.find('{')\n",
        "    end_index = text.rfind('}')  # Get the last closing brace\n",
        "\n",
        "    if start_index == -1 or end_index == -1 or end_index < start_index:\n",
        "        return None  # Return None if valid JSON braces are not found\n",
        "\n",
        "    # Extract the substring that forms the JSON object\n",
        "    json_str = text[start_index:end_index + 1]\n",
        "\n",
        "    # Replace newline characters and other potential issues\n",
        "    #json_str = json_str.replace('\\n', '\\\\n').replace('\\r', '\\\\r').replace('\\t', '\\\\t')\n",
        "\n",
        "    return json_str\n",
        "\n",
        "def format_output(text: str):\n",
        "    print_debug_message(f\"format_output(): initial text: {text}\\n\")\n",
        "    full_json_str = extract_first_json(text)\n",
        "\n",
        "    if full_json_str is None:\n",
        "        print_debug_message(f\"format_output(): No valid JSON found calling extract_first_json() with text - {text}\")\n",
        "        return None\n",
        "\n",
        "    print_debug_message(f\"format_output(): full_json_str after extract_first_json()call - {full_json_str}\\n\")\n",
        "\n",
        "    try:\n",
        "        return json.loads(full_json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print_debug_message(f\"format_output(): Error decoding JSON from json.loads(full_json_str) with text - {text}\\nand full_json_str - {full_json_str}\\nError Message - {e}\")\n",
        "        return None  # Handle the error as needed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Processes the action dict created by the format_output() function to execute\n",
        "# the selected tool or provide the AI's final response based on the value of\n",
        "# action[\"tool_name\"] (if tool name isn't recognized, it will assume it's a\n",
        "# \"Final Answer\" action)\n",
        "def use_tool(action: dict):\n",
        "    tool_name = action[\"tool_name\"]\n",
        "    if tool_name == \"Final Answer\":\n",
        "        is_tool_response = False\n",
        "        return f\"\\n\\nAI Assistant: {action['input']}\", is_tool_response\n",
        "    elif tool_name == \"Calculator\":\n",
        "        print(\"\\nUsing Calculator...\\n\")\n",
        "        # Create a dictionary to serve as a local namespace for exec\n",
        "        local_namespace = {}\n",
        "\n",
        "        # Execute the code within the local namespace\n",
        "        exec(action[\"input\"], {}, local_namespace)\n",
        "\n",
        "        # Access the value of 'result' from the local namespace\n",
        "        exec_result = local_namespace.get('output', None)\n",
        "        is_tool_response = True\n",
        "        return f\"\\n\\nTool Output: {exec_result}\", is_tool_response\n",
        "    elif tool_name == \"Search\":\n",
        "        print(f\"\\nSearching the Web for {action['input']}...\\n\")\n",
        "        contexts = []\n",
        "        with DDGS() as ddgs:\n",
        "            results = ddgs.text(\n",
        "                action[\"input\"],\n",
        "                region=\"wt-wt\", safesearch=\"on\",\n",
        "                max_results=3\n",
        "            )\n",
        "            for r in results:\n",
        "                contexts.append(r['body'])\n",
        "        info = \"\\n---\\n\".join(contexts)\n",
        "        is_tool_response = True\n",
        "        return f\"\\n\\nTool Output: {info}\", is_tool_response\n",
        "    else:\n",
        "        # otherwise just assume final answer\n",
        "        is_tool_response = False\n",
        "        return f\"\\n\\nAI Assistant: {action['input']}\", is_tool_response\n",
        "\n",
        "# Takes a full input_prompt (with prior conversation history included),\n",
        "# queries the Mixtral model, processes the text-generation response, and\n",
        "# itteratively executes Tool Usage until a \"Final Answer\" answer is recieved.\n",
        "# Then, returns both the response_message to be displayed and the\n",
        "# new input_prompt with all new messages appended\n",
        "def handle_message(input_prompt):\n",
        "\n",
        "  response = generate_text(input_prompt)\n",
        "  generated_text = response[0]['generated_text']\n",
        "  print_debug_message(f\"handle_message(): initial generated_text: {generated_text}\\n\")\n",
        "\n",
        "  # If it fails to load json, it probably just responded\n",
        "  # directly without the dict, so build a dict out of it\n",
        "  # in a try/except clause\n",
        "  try:\n",
        "      action = format_output(generated_text)\n",
        "      if action is None:\n",
        "        # if action is None, the formatting was wrong, meaning the model probably didn't\n",
        "        # wrap its response message in an action dict, so raise an error to let except\n",
        "        # block reformat it into a \"Final Answer\" action dict json string\n",
        "        raise Exception(\"Failed to parse json from generated_text to creare action dict.\")\n",
        "      response_message, is_tool_response = use_tool(action)\n",
        "  except Exception as e:\n",
        "\n",
        "      # If formatting the output and using the tool fails, then the model\n",
        "      # probably didn't use action dict json string formatting, so just\n",
        "      # assume its message was intended to be a direct message to the user\n",
        "      # and reformat it as a \"Final Answer\" action\n",
        "      print_debug_message(f\"handle_message(): Exception Block triggered for first text-generation's format_output()/use_tool() call! Exception - {e}\")\n",
        "\n",
        "      # Since the model probably sent direct response message rather than formatting it as\n",
        "      # an action dict json string, we need to remove any newlines, whitespace, etc\n",
        "      # to ensure the new action dict input value is json parsable in the new_generated_text\n",
        "      # json string\n",
        "      generated_text = sanitize_text_for_json(text = generated_text)\n",
        "      # Now format the response into a valid action dict json string\n",
        "      new_generated_text = \"\"\"\n",
        "      ```json\n",
        "      {\n",
        "          \"tool_name\": \"Final Answer\",\n",
        "          \"input\": \"\"\" + f\"\\\"{generated_text}\\\"\"+ \"\"\"\n",
        "      }\n",
        "      ```\n",
        "      \"\"\"\n",
        "      generated_text = new_generated_text\n",
        "      action = format_output(generated_text)\n",
        "      response_message, is_tool_response = use_tool(action)\n",
        "  # Add Initial Assistant Response to the prompt\n",
        "  input_prompt += generated_text\n",
        "\n",
        "  # If response is tool response, add the response message\n",
        "  # to the prompt and query again, return the full new\n",
        "  # input_prompt along with the response_message to be displayed\n",
        "  # to the user\n",
        "  while is_tool_response:\n",
        "    input_prompt += response_message + \"\\n\\nAI Assistant: \"\n",
        "    response = generate_text(input_prompt)\n",
        "    generated_text = response[0]['generated_text']\n",
        "    print_debug_message(f\"handle_message(): while is_tool_response generated_text : {generated_text}\")\n",
        "\n",
        "    # If it fails to load json, it probably just responded\n",
        "    # directly without the dict, so build a dict out of it\n",
        "    # in a try/except clause\n",
        "    try:\n",
        "        action = format_output(generated_text)\n",
        "        if action is None:\n",
        "          # if action is None, the formatting was wrong, meaning the model probably didn't\n",
        "          # wrap its response message in an action dict, so raise an error to let except\n",
        "          # block reformat it into a \"Final Answer\" action dict json string\n",
        "          raise Exception(\"Failed to parse json from generated_text to creare action dict.\")\n",
        "        response_message, is_tool_response = use_tool(action)\n",
        "    except Exception as e:\n",
        "\n",
        "        # If formatting the output and using the tool fails, then the model\n",
        "        # probably didn't use action dict json string formatting, so just\n",
        "        # assume its message was intended to be a direct message to the user\n",
        "        # and reformat it as a \"Final Answer\" action\n",
        "        print_debug_message(f\"handle_message(): Exception Block triggered for 'while is_tool_response' loop format_output()/use_tool() call! Exception - {e}\")\n",
        "\n",
        "        # Since the model probably sent direct response message rather than formatting it as\n",
        "        # an action dict json string, we need to remove any newlines, whitespace, etc\n",
        "        # to ensure the new action dict input value is json parsable in the new_generated_text\n",
        "        # json string\n",
        "        generated_text = sanitize_text_for_json(text = generated_text)\n",
        "        # Now format the response into a valid action dict json string\n",
        "        new_generated_text = \"\"\"\n",
        "        ```json\n",
        "        {\n",
        "            \"tool_name\": \"Final Answer\",\n",
        "            \"input\": \"\"\" + f\"\\\"{generated_text}\\\"\"+ \"\"\"\n",
        "        }\n",
        "        ```\n",
        "        \"\"\"\n",
        "        generated_text = new_generated_text\n",
        "        action = format_output(generated_text)\n",
        "        response_message, is_tool_response = use_tool(action)\n",
        "    # Add the new is_tool_response_loop generated_text to the prompt\n",
        "    input_prompt += generated_text\n",
        "\n",
        "\n",
        "\n",
        "  return response_message, input_prompt\n",
        "\n",
        "# Initializes AI Chat orchestrating user inputs and multi-step chat logic\n",
        "def run_ai_chat(sys_message: str = None, max_user_messages: int = 10, chat_log_dir: str = \"logs/\"):\n",
        "\n",
        "    # Create Chat Log Filepath to save convo history\n",
        "    now = datetime.datetime.now()\n",
        "    formatted_datetime = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "    if not os.path.exists(chat_log_dir):\n",
        "      os.makedirs(chat_log_dir)\n",
        "    chat_log_file = f\"{chat_log_dir}User_Conversation_{formatted_datetime}.txt\"\n",
        "\n",
        "    # Begin AI Assistant Chat\n",
        "    output.clear()\n",
        "    print(\"\\n\\nBeginning AI Assistant Chat. Type 'exit' at any time to end the chat\\n\\n\\n\")\n",
        "    print(\"\\n\\n\")\n",
        "    user_message = input(\"User: \")\n",
        "    print(\"\\n\\n\")\n",
        "    input_prompt = first_prompt_instruction_format(user_message, sys_message = sys_message)\n",
        "    response_message, input_prompt = handle_message(\n",
        "                                          input_prompt\n",
        "                                        )\n",
        "\n",
        "    print(response_message)\n",
        "    print(\"\\n\\n\")\n",
        "    # Save Chat Log\n",
        "    with open(chat_log_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(input_prompt)\n",
        "    # Start at 1, not 0, cuz they already sent the first message\n",
        "    for i in range(1, max_user_messages):\n",
        "      print(\"\\n\\n\")\n",
        "      user_message = input(\"User: \")\n",
        "      print(\"\\n\\n\")\n",
        "      input_prompt += \"\\nUser: \" + user_message\n",
        "\n",
        "      if user_message.upper() == 'EXIT':\n",
        "        input_prompt += \"\\n\\n\\nChat Ended. Have a great day! (:\"\n",
        "        print(\"\\n\\n\\nChat Ended. Have a great day! (:\\n\\n\")\n",
        "        # Save Chat Log\n",
        "        with open(chat_log_file, 'w', encoding='utf-8') as file:\n",
        "            file.write(input_prompt)\n",
        "        return input_prompt\n",
        "      input_prompt += f\"\\nUser: {user_message}\"\n",
        "      response_message, input_prompt = handle_message(\n",
        "                                          input_prompt\n",
        "                                        )\n",
        "\n",
        "      print(response_message)\n",
        "      print(\"\\n\\n\")\n",
        "      # Save Chat Log\n",
        "      with open(chat_log_file, 'w', encoding='utf-8') as file:\n",
        "            file.write(input_prompt)\n",
        "\n",
        "    print(\"\\n\\n\\nChat Ended. Have a great day! (:\")\n",
        "    # Save Chat Log\n",
        "    with open(chat_log_file, 'w', encoding='utf-8') as file:\n",
        "            file.write(input_prompt)\n",
        "    return input_prompt\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\nMixtral-8x7B-Instruct Model Loaded Successfully!\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tcFSb5rejKzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Run Mixtral AI Chat\n",
        "\n",
        "Once the Mixtral model is downloaded and initialized, execute the cell below to begin your chat session.\n",
        "\n",
        "### Chat Session Settings\n",
        "- `max_user_messages: int = 20`: Sets the limit for user messages in a single AI Chat session. This helps to maintain conversations within the model's maximum sequence length.\n",
        "- `debug_mode: bool = False`: Toggle this to `True` to enable the display of debug messages.\n"
      ],
      "metadata": {
        "id": "rfVLoMBRxlpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run Mixtral8x7B AI Chat with Tools\n",
        "\n",
        "# How many total messages the user can send in a given AI Chat session\n",
        "# before it automatically ends the conversation. This prevents\n",
        "# conversations from becoming too long to fit within the model's maximum\n",
        "# sequence length\n",
        "max_user_messages = 20 # @param {type:\"integer\"}\n",
        "debug_mode = False # @param {type:\"boolean\"}\n",
        "\n",
        "# Set sys_message to None to use default tool-enabled system prompt (default\n",
        "# system prompt is located in the first_prompt_instruction_format() function\n",
        "# of the \"Download & Initialize Mixtral8x7B Model\" cell above)\n",
        "sys_message = None, # Setting \"None\"\n",
        "\n",
        "# Set \"DEBUG\" env variable to toggle print_debug_message() function printing\n",
        "if debug_mode:\n",
        "  os.environ['DEBUG'] = 'TRUE'\n",
        "else:\n",
        "  os.environ['DEBUG'] = 'FALSE'\n",
        "\n",
        "# Creates a chat_log_dir directory to save a .txt file of the full\n",
        "# chat conversation for each AI Chat session (including system prompt,\n",
        "# Tool Calls, Tool Outputs, etc) to help with debugging\n",
        "chat_log_dir = \"logs/\"\n",
        "\n",
        "# Set the logging level to suppress warnings\n",
        "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
        "logging.getLogger('bitsandbytes').setLevel(logging.ERROR)\n",
        "\n",
        "input_prompt = run_ai_chat(\n",
        "                  sys_message = sys_message,\n",
        "                  max_user_messages = max_user_messages,\n",
        "                  chat_log_dir = chat_log_dir\n",
        "                )\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SBUFYBDuxcgw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "A100",
      "machine_shape": "hm",
      "collapsed_sections": [
        "TPVUrcqKkvuG",
        "AQ5nkCe8h35L",
        "rfVLoMBRxlpp"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
